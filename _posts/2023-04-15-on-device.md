---
title: "Balacoon TTS on-device"
last_modified_at: 2023-04-15T16:20:02-05:00
toc: true
categories:
  - Blog
tags:
  - text-to-speech
  - on-device
---

Neural text-to-speech brought unprecedented improvements in the naturalness of synthetic speech. But it came with a cost. While parametric and concatenative speech synthesis systems produce tens of seconds of audio in just 1 second of wall time (they deliver >10 xRT[*](#0)) on a single CPU core, neural TTS requires way more computational power. You often need a GPU to provide compelling latency for responsive applications. Fortunately, when there is a will, there is a way. Let's dive into on-device Neural TTS and see what Balacoon has to offer.

## On-device Neural TTS recap
Several milestones of Neural TTS evolution are worth mentioning in this regard. Generating raw waveform is the most computationally expensive part of synthesis. WaveRNN[[1]](#1) from Google pioneered real-time synthesis on CPU. The authors used sparsification (dropping most neural network weights) and subscaling (generating multiple samples simultaneously) to achieve remarkable results. Later LPCNet[[2]](#2) brought these advances, as well as an idea of mixing signal processing with neural networks, to the public. And finally, in a trend of GAN-based vocoding overtaking the domain, MB-MelGAN[[3]](#3) came forward by breaking the curse of auto-regressive waveform generation.

Acoustic features prediction was a less acute problem and down-scaled reasonably well. The most widely spread FastSpeech2[[4]](#4) already has only 30M parameters and runs reasonably fast. And with LightSpeech[[5]](#5), Microsoft has shown that it is possible to shrink it down to 2M parameters.

So once VITS[[6]](#6) and JETS[[7]](#7) paved the way to end-to-end speech synthesis, combining acoustic features prediction and vocoding, it was already clear that low resource end-to-end TTS is just around the corner. Indeed NIX-TTS[[8]](#8) came into the game, squishing the whole Neural TTS backend into 5M parameters that run 0.5xRT on a Raspberry PI 3B.

## Implementations available
While [LPCNet](https://github.com/xiph/LPCNet) is not so widely used anymore, it is worth mentioning because the implementation contains valuable engineering insights, such as sparsification and vectorization. TensorFlowTTS combines mentioned FastSpeech2 and MB-MelGAN in an [android example](https://github.com/TensorSpeech/TensorFlowTTS/tree/master/examples/android) powered by TFLite. Nix-TTS authors release their [code and models](https://github.com/rendchevi/nix-tts). And lastly, there is [Piper](https://github.com/rhasspy/piper), which competes with Nix-TTS in terms of performance (also 5M parameters models), but instead of distillation, it simply downscales VITS architecture.

## Introducing LightðŸ’¡
We composed our own version of the lightweight TTS model called **Light**. It has fewer parameters compared to default JETS models. Therefore it compromises quality and multi-speaker, multi-lingual capabilities. It also delivers only 16kHz audio instead of 24kHz. On the other hand, **it provides an order of magnitude faster synthesis on the CPU**.

Degradation compared to full-scale model on the held-out test set of "92" Hi-Fi speaker:

| Model | [Naturalness](https://arxiv.org/abs/2204.02152) (MOSâ†‘)  | [Intelligibility](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_en_conformer_transducer_xlarge) (CER, %â†“) |
| --- | :---: | :---: |
| recordings | 3.92 | 0.32 |
| en_us_hifi_jets_cpu.addon | 4.0 | 0.28 |
| en_us_hifi92_light_cpu.addon | 3.89 | 0.32 |


Synthesis speed on AMD Ryzen Threadripper 1950X:

| Model/System | faster than real-time (xRTâ†‘) |
| --- | :---: |
| en_us_hifi_jets_cpu.addon | 6.02 |
| [Piper (ljspeech)](https://github.com/rhasspy/piper) | 29.15 |
| en_us_hifi92_light_cpu.addon | **50.86** |

You can try out `en_us_hifi92_light_cpu.addon` in our [huggingface space](https://huggingface.co/spaces/balacoon/tts) and use it with `balacoon_tts` python package as described in a [tutorial](https://balacoon.com/use/tts/package).

Stay tuned for the release of RaspberryPI-compatible `balacoon_tts` version and pre-compiled libs for Android, so you can utilize LightðŸ’¡ potential in your off-the-grid projects.

## References
<a id="1">[1]</a>
[Efficient Neural Audio Synthesis](https://arxiv.org/pdf/1802.08435.pdf)

<a id="2">[2]</a>
[LPCNet: Improving Neural speech synthesis through linear prediction](https://jmvalin.ca/papers/lpcnet_icassp2019.pdf)

<a id="3">[3]</a>
[Multi-Band MelGAN: Faster waveform generation for high-quality Text-to-Speech](https://arxiv.org/pdf/2005.05106.pdf)

<a id="4">[4]</a>
[FastSpeech 2: Fast and High-Quality End-to-End Text to Speech](https://arxiv.org/abs/2006.04558)

<a id="5">[5]</a>
[LightSpeech: Lightweight and fast Text-to-Speech with Neural Architecture Search](https://arxiv.org/pdf/2102.04040.pdf)

<a id="6">[6]</a>
[Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech](https://arxiv.org/abs/2106.06103)

<a id="7">[7]</a>
[JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech](https://arxiv.org/abs/2203.16852)

<a id="8">[8]</a>
[NIX-TTS: Lightweight and end-to-end Text-to-Speech via module-wise distillation](https://arxiv.org/pdf/2203.15643.pdf)

***

<a id="0">*</a>
There is a certain confusion around "xRT" (times real-time) terminology. Some people mean "how much audio is produced in one second of walltime"; others refer to "how much time it takes to synthesize one second of audio". While the latter is generally more popular, I stick with the former because numbers like "30xRT" and "50xRT" are easier to comprehend and compare than "0.033xRT" and "0.02xRT".
