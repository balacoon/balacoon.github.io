{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running mHubert using transformers\n",
    "\n",
    "Here we run mHubert using transformers abstractions and apply the quantization on top.\n",
    "We use model after 2nd iteration which is slightly worse. But in that way we can utilize released faiss index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow docs https://huggingface.co/docs/transformers/en/model_doc/hubert#transformers.HubertModel.forward.example\n",
    "# to extract latent representation for the audio file after 9th layer\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import HubertModel\n",
    "import soundfile as sf\n",
    "import faiss\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = HubertModel.from_pretrained(\"utter-project/mHuBERT-147-base-2nd-iter\").cuda()\n",
    "    # Specify the repository and the filename you want to download\n",
    "    repo_id = \"utter-project/mHuBERT-147\"\n",
    "    filename = \"mhubert147_faiss.index\"\n",
    "    download_dir = \"./\"\n",
    "\n",
    "    # Download the file to the specified directory\n",
    "    index_path = download_dir + filename\n",
    "    if not os.path.isfile(index_path):\n",
    "        index_path = hf_hub_download(repo_id=repo_id, filename=filename, cache_dir=download_dir)\n",
    "    index: faiss.IndexPreTransform = faiss.read_index(index_path)\n",
    "    index_ivf = faiss.extract_index_ivf(index)\n",
    "    return model, index, index_ivf, index_path\n",
    "\n",
    "\n",
    "def extract_latent(model, audio_path):\n",
    "    wav, sr = sf.read(audio_path)\n",
    "    assert sr == 16000\n",
    "    x = torch.tensor(wav).unsqueeze(0).float().cuda()\n",
    "    # need to do mean / variance normalization\n",
    "    x = (x - x.mean()) / (torch.sqrt(x.var() + 1e-7))\n",
    "    # https://huggingface.co/utter-project/mHuBERT-147/discussions/6#668544a0e270025784fb469c\n",
    "    latent = model(x, output_hidden_states=True).hidden_states[9]\n",
    "    return latent # 1 x time x 768\n",
    "\n",
    "\n",
    "def extract_labels_from_latent(index, index_ivf, latent):\n",
    "    xq = latent.reshape(latent.size(0) * latent.size(1), -1).float().detach().cpu().numpy()\n",
    "    opq_mt = faiss.downcast_VectorTransform(index.chain.at(0))\n",
    "    #Apply pre-transform to query\n",
    "    xq_t = opq_mt.apply_py(xq)\n",
    "    #Get centroids C and distances DC on a pre-transformed index\n",
    "    _, C = index_ivf.quantizer.search(xq_t, 1)\n",
    "    return C[:, 0]  # (time)\n",
    "\n",
    "\n",
    "def extract_labels(model, index, index_ivf, audio_path):\n",
    "    latent = extract_latent(model, audio_path)\n",
    "    labels = extract_labels_from_latent(index, index_ivf, latent)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, index, index_ivf, index_path = get_model()\n",
    "this_path = \"rms_arctic_a0001.wav\"\n",
    "orig_labels = extract_labels(model, index, index_ivf, this_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing mHubert\n",
    "\n",
    "Below we implement mHubert together with quantization as a torch module, so it can be traced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class TorchFaiss(nn.Module):\n",
    "    def __init__(self, index_path):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load FAISS index\n",
    "        index = faiss.read_index(index_path)\n",
    "        index_ivf = faiss.extract_index_ivf(index)\n",
    "\n",
    "        # get the transformation\n",
    "        vt = index.chain.at(0)\n",
    "        # extracting transformation, works only for a linear one\n",
    "        I = np.eye(vt.d_in, dtype=np.float32)\n",
    "        transform_mat = vt.apply_py(I)\n",
    "        self.transform = nn.Linear(transform_mat.shape[0], transform_mat.shape[1], bias=False)\n",
    "        self.transform.weight = nn.Parameter(torch.tensor(transform_mat).T)\n",
    "        self.transform.requires_grad_(False)\n",
    "\n",
    "        # get the index search\n",
    "        nlist = index_ivf.nlist  # Number of clusters\n",
    "        d = index_ivf.d  # Vector dimension\n",
    "        centroids_npy = np.zeros((nlist, d), dtype=np.float32)\n",
    "        index_ivf.quantizer.reconstruct_n(0, nlist, centroids_npy)\n",
    "        centroids_arr = torch.tensor(centroids_npy)\n",
    "       \n",
    "        self.centroids_norm = nn.Parameter((centroids_arr ** 2).sum(dim=1, keepdim=True).T)\n",
    "        self.centroids_norm.requires_grad_(False)\n",
    "        self.centroids = nn.Linear(centroids_npy.shape[1], centroids_arr.shape[0], bias=False)\n",
    "        self.centroids.weight = nn.Parameter(centroids_arr)\n",
    "        self.centroids.requires_grad_(False)\n",
    "      \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: torch.Tensor (batch x time x dim)\n",
    "        latent to convert to centroid indices\n",
    "\n",
    "        labels: torch.Tensor(batch x time)\n",
    "        \"\"\"\n",
    "        x = self.transform(x)\n",
    "        x_norm = (x ** 2).sum(dim=2, keepdim=True)  # (batch x time x 1)\n",
    "        logits = self.centroids(x)\n",
    "        dists = x_norm + self.centroids_norm - 2 * logits\n",
    "        _, labels = torch.min(dists, dim=2)\n",
    "        return labels\n",
    "\n",
    "\n",
    "class HubertExtractor(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper around hubert extractor, that combines\n",
    "    hubert model and discretization using kmeans.\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_repo_id, index_path, full_precision: bool = True):\n",
    "        super().__init__()\n",
    "        self._float_type = torch.float32 if full_precision else torch.float16\n",
    "        self._model = HubertModel.from_pretrained(hf_repo_id)\n",
    "        self._faiss = TorchFaiss(index_path)\n",
    "\n",
    "    def _compute_stats(self, audio: torch.Tensor, audio_len: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Computes mean and std taking into account padding in the sequences\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mean: torch.Tensor\n",
    "            (batch,) corrected mean, per sequence in batch as if sequences were not padded\n",
    "        std: torch.Tensor\n",
    "            (batch,) corrected standard deviation as if sequences were not padded\n",
    "        \"\"\"\n",
    "        total_len = torch.ones_like(audio_len) * audio.size(1)\n",
    "        correction = (total_len / audio_len).type(self._float_type)\n",
    "        pad_len = total_len - audio_len\n",
    "\n",
    "        mean = torch.mean(audio, dim=1)\n",
    "        # apply correction, replacing denominator\n",
    "        mean = mean * correction\n",
    "\n",
    "        # this variance formula corresponds to torch.var(unbiased=False) or ddof=0 in numpy\n",
    "        var = torch.mean(torch.square(audio - mean.unsqueeze(1)), dim=1)\n",
    "        # subtract extra values that are added in padding regions\n",
    "        var = var - pad_len * (torch.square(mean) / total_len)\n",
    "        # apply correction, replacing denominator\n",
    "        var = var * correction\n",
    "\n",
    "        return mean, torch.sqrt(var + 1e-7)\n",
    "\n",
    "    def _create_mask(self, audio: torch.Tensor, audio_len: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Creates binary mask with \"0\" for padded regions.\n",
    "        Used to zerofy padding after normalization\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mask: torch.Tensor\n",
    "            (batch, samples_num) - binary mask with \"0\" for padded regions. can be multiplied\n",
    "            with audio to zerofy padded regions\n",
    "        \"\"\"\n",
    "        max_len = audio.size(1)\n",
    "        ranged = torch.arange(max_len, device=audio_len.device, dtype=audio_len.dtype)\n",
    "        batch_ranged = ranged.expand(audio_len.size(0), max_len)\n",
    "        mask = batch_ranged < audio_len.unsqueeze(1)\n",
    "        return mask\n",
    "\n",
    "    def _mean_var_norm(self, audio: torch.Tensor, audio_len: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Implements traceable version of mean_var_norm from Wav2Vec2FeatureExtractor:\n",
    "        https://github.com/huggingface/transformers/blob/31d452c68b34c2567b62924ee0df40a83cbc52d5/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py#L81\n",
    "        \"\"\"\n",
    "        mean, std = self._compute_stats(audio, audio_len)\n",
    "        audio = (audio - mean.unsqueeze(1)) / std.unsqueeze(1)\n",
    "        # need to make padding zero after normalization\n",
    "        mask = self._create_mask(audio, audio_len)\n",
    "        audio = audio * mask\n",
    "        return audio\n",
    "\n",
    "    def forward(self, audio: torch.Tensor, audio_len: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extracts labels from raw audio. Runs inference with pre-trained hubert model,\n",
    "        checks which centroid is closest for each frame's continuous representation\n",
    "        and returns its id.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        audio: torch.Tensor\n",
    "            (batch_size x samples_num) - audio samples in int16 in range (-INT_MAX; INT_MAX)\n",
    "        audio_len: torch.Tensor\n",
    "            (batch_size,) - input that specifies actual length of audio within batch.\n",
    "            Should be int32, stored on CPU regardless of config (cuda, half)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        labels: torch.Tensor\n",
    "            (batch_size x frames_num) - pseudo annotation for audio on frame level.\n",
    "        \"\"\"\n",
    "        # convert audio from short to float\n",
    "        audio = audio.type(self._float_type) / 32768.0  # to range (-1, 1)\n",
    "\n",
    "        # run mean/var normalization\n",
    "        audio = self._mean_var_norm(audio, audio_len)\n",
    "\n",
    "        # Attention does not really affect\n",
    "        # `attention_mask` note in https://huggingface.co/docs/transformers/en/model_doc/hubert#transformers.HubertModel.forward\n",
    "        #attention_mask = torch.arange(\n",
    "        #    audio.size(1),\n",
    "        #    device=audio.device\n",
    "        #)[None, :] < audio_len[:, None]\n",
    "        #attention_mask = attention_mask.long()\n",
    "\n",
    "        # run inference with transformer\n",
    "        emb = self._model(audio, output_hidden_states=True).hidden_states[9]\n",
    "\n",
    "        # compute labels\n",
    "        labels = self._faiss(emb)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracing in half precision works, but model mysteriously fails on longer inputs\n",
    "extractor = HubertExtractor(\"utter-project/mHuBERT-147-base-2nd-iter\", index_path, full_precision=False).cuda().half()\n",
    "wav, sr = sf.read(\n",
    "    \"rms_arctic_a0001.wav\",\n",
    "    dtype=\"int16\"\n",
    ")\n",
    "assert sr == 16000\n",
    "x = torch.tensor(wav).unsqueeze(0).cuda()\n",
    "x_len = torch.tensor([x.shape[1]]).cuda()\n",
    "trace_labels = extractor(x, x_len).detach().cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "print(orig_labels.shape)\n",
    "print(trace_labels.shape)\n",
    "\n",
    "plt.plot(orig_labels)\n",
    "plt.plot(trace_labels, alpha=0.5)\n",
    "plt.legend([\"original\", \"traced\"])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace the extractor and save jit\n",
    "import os\n",
    "\n",
    "out_path = \"./mhubert147_fp16_cuda.jit\"\n",
    "for param in extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "with torch.no_grad():\n",
    "        # trace the model and save result\n",
    "        extractor_traced = torch.jit.trace(extractor, [x, x_len])\n",
    "        extractor_traced.save(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running batched inference\n",
    "\n",
    "Check how much padding affects extracted hubert labels.\n",
    "There is discrepancy expected, because attention mask is not really utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check that batching works\n",
    "dtype = \"int16\"\n",
    "wav1, _ = sf.read(\"slt_arctic_a0001.wav\", dtype=dtype)\n",
    "# rms is shorter and will be padded\n",
    "wav2, _ = sf.read(\"rms_arctic_a0001.wav\", dtype=dtype)\n",
    "# Convert to tensors\n",
    "wav1 = torch.tensor(wav1)\n",
    "wav2 = torch.tensor(wav2)\n",
    "x_len = torch.tensor([wav1.shape[0], wav2.shape[0]]).cuda()\n",
    "# Find max length and pad\n",
    "max_len = max(wav1.shape[0], wav2.shape[0])\n",
    "print(f\"padding {wav1.shape[0]} and {wav2.shape[0]} to {max_len}\")\n",
    "wav1 = torch.nn.functional.pad(wav1, (0, max_len - wav1.shape[0]))\n",
    "wav2 = torch.nn.functional.pad(wav2, (0, max_len - wav2.shape[0]))\n",
    "# Stack and move to CUDA\n",
    "x = torch.stack([wav1, wav2]).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "traced_extractor = torch.jit.load(out_path).to(torch.device(\"cuda\"))\n",
    "batched_labels = traced_extractor(x, x_len)[1].detach().cpu().numpy()\n",
    "\n",
    "print(orig_labels.shape)\n",
    "print(batched_labels.shape)\n",
    "\n",
    "plt.plot(orig_labels)\n",
    "plt.plot(batched_labels, alpha=0.5)\n",
    "plt.legend([\"original\", \"batch\"])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload traced model\n",
    "\n",
    "Upload the traced model to the HF repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import upload_file\n",
    "\n",
    "upload_file(\n",
    "    path_or_fileobj=out_path,\n",
    "    path_in_repo=os.path.basename(out_path),\n",
    "    repo_id=\"balacoon/mhubert-147\",\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "borysthenes_vocoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
